{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVjZFPUfWf37",
        "outputId": "eef2afd6-dd34-4836-f468-73cb55914dd8"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow numpy pandas scikit-learn xgboost lightgbm shap joblib scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XIZ6GkWWlGQ",
        "outputId": "f4d1a44e-0a14-4ff9-e06e-e4da23689775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.4.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.22.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow numpy pandas scikit-learn xgboost lightgbm shap joblib scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BclF43RCWO6N"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVC\n",
        "import shap\n",
        "import os\n",
        "import joblib\n",
        "from scipy.sparse import issparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teO3zkW9jhpw",
        "outputId": "bfa4f649-a06b-4df2-f2c3-1bc332752ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.4.0)\n",
            "Collecting shap\n",
            "  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: slicer, nvidia-nccl-cu12, shap\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.22.3 shap-0.46.0 slicer-0.0.8\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow numpy pandas scikit-learn xgboost lightgbm shap joblib scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wNeG7QYWHsg",
        "outputId": "bdd1b922-0efd-4494-d202-d31f3b0c3eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 267ms/step - accuracy: 0.5445 - loss: 0.7206 - val_accuracy: 0.5630 - val_loss: 0.6874 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6094 - loss: 0.6761 - val_accuracy: 0.5704 - val_loss: 0.6848 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6190 - loss: 0.6554 - val_accuracy: 0.6037 - val_loss: 0.6631 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6394 - loss: 0.6433 - val_accuracy: 0.6000 - val_loss: 0.6626 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6311 - loss: 0.6374 - val_accuracy: 0.6000 - val_loss: 0.6667 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.6617 - loss: 0.6226 - val_accuracy: 0.6296 - val_loss: 0.6610 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6732 - loss: 0.6122 - val_accuracy: 0.6074 - val_loss: 0.6529 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6843 - loss: 0.5952 - val_accuracy: 0.6259 - val_loss: 0.6494 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7041 - loss: 0.5799 - val_accuracy: 0.6185 - val_loss: 0.6474 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7060 - loss: 0.5811 - val_accuracy: 0.6185 - val_loss: 0.6422 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7020 - loss: 0.5686 - val_accuracy: 0.6259 - val_loss: 0.6437 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7117 - loss: 0.5691 - val_accuracy: 0.6222 - val_loss: 0.6507 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7439 - loss: 0.5431 - val_accuracy: 0.6222 - val_loss: 0.6500 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7342 - loss: 0.5369 - val_accuracy: 0.6222 - val_loss: 0.6490 - learning_rate: 1.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7427 - loss: 0.5344 - val_accuracy: 0.6185 - val_loss: 0.6484 - learning_rate: 1.0000e-04\n",
            "[LightGBM] [Info] Number of positive: 876, number of negative: 922\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005153 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 12379\n",
            "[LightGBM] [Info] Number of data points in the train set: 1798, number of used features: 171\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487208 -> initscore=-0.051179\n",
            "[LightGBM] [Info] Start training from score -0.051179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - accuracy: 0.4966 - loss: 0.8295 - val_accuracy: 0.4500 - val_loss: 0.7467 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5089 - loss: 0.7336 - val_accuracy: 0.5083 - val_loss: 0.7031 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5703 - loss: 0.6839 - val_accuracy: 0.5833 - val_loss: 0.6881 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5685 - loss: 0.6886 - val_accuracy: 0.6111 - val_loss: 0.6820 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5890 - loss: 0.6790 - val_accuracy: 0.6278 - val_loss: 0.6778 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5715 - loss: 0.6850 - val_accuracy: 0.6139 - val_loss: 0.6741 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6023 - loss: 0.6727 - val_accuracy: 0.6056 - val_loss: 0.6718 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6047 - loss: 0.6653 - val_accuracy: 0.5917 - val_loss: 0.6709 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5966 - loss: 0.6723 - val_accuracy: 0.6083 - val_loss: 0.6687 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6195 - loss: 0.6544 - val_accuracy: 0.6222 - val_loss: 0.6654 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6279 - loss: 0.6541 - val_accuracy: 0.6361 - val_loss: 0.6619 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6296 - loss: 0.6479 - val_accuracy: 0.6389 - val_loss: 0.6591 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6594 - loss: 0.6364 - val_accuracy: 0.6417 - val_loss: 0.6557 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6521 - loss: 0.6383 - val_accuracy: 0.6417 - val_loss: 0.6530 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6532 - loss: 0.6381 - val_accuracy: 0.6472 - val_loss: 0.6510 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6743 - loss: 0.6268 - val_accuracy: 0.6556 - val_loss: 0.6496 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6568 - loss: 0.6279 - val_accuracy: 0.6611 - val_loss: 0.6487 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6550 - loss: 0.6271 - val_accuracy: 0.6500 - val_loss: 0.6481 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6754 - loss: 0.6237 - val_accuracy: 0.6500 - val_loss: 0.6487 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6764 - loss: 0.6055 - val_accuracy: 0.6472 - val_loss: 0.6512 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6824 - loss: 0.6078 - val_accuracy: 0.6417 - val_loss: 0.6512 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6906 - loss: 0.6028 - val_accuracy: 0.6472 - val_loss: 0.6509 - learning_rate: 1.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6757 - loss: 0.5997 - val_accuracy: 0.6500 - val_loss: 0.6506 - learning_rate: 1.0000e-04\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Epoch 1/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.4956 - loss: 0.7363 - val_accuracy: 0.5583 - val_loss: 0.7057 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5007 - loss: 0.7122 - val_accuracy: 0.5583 - val_loss: 0.6915 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5348 - loss: 0.6930 - val_accuracy: 0.9500 - val_loss: 0.6779 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9079 - loss: 0.6750 - val_accuracy: 0.6639 - val_loss: 0.6679 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6636 - loss: 0.6625 - val_accuracy: 0.5583 - val_loss: 0.6608 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5696 - loss: 0.6522 - val_accuracy: 0.5111 - val_loss: 0.6543 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5387 - loss: 0.6430 - val_accuracy: 0.4917 - val_loss: 0.6481 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5226 - loss: 0.6368 - val_accuracy: 0.4917 - val_loss: 0.6418 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5251 - loss: 0.6280 - val_accuracy: 0.5028 - val_loss: 0.6354 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5374 - loss: 0.6188 - val_accuracy: 0.5194 - val_loss: 0.6287 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5422 - loss: 0.6140 - val_accuracy: 0.5556 - val_loss: 0.6211 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5796 - loss: 0.6017 - val_accuracy: 0.5861 - val_loss: 0.6121 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6071 - loss: 0.5927 - val_accuracy: 0.6222 - val_loss: 0.6007 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6391 - loss: 0.5863 - val_accuracy: 0.6917 - val_loss: 0.5866 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7097 - loss: 0.5682 - val_accuracy: 0.7500 - val_loss: 0.5713 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7718 - loss: 0.5555 - val_accuracy: 0.8056 - val_loss: 0.5584 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8202 - loss: 0.5483 - val_accuracy: 0.8444 - val_loss: 0.5474 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8579 - loss: 0.5373 - val_accuracy: 0.8528 - val_loss: 0.5363 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8742 - loss: 0.5259 - val_accuracy: 0.8639 - val_loss: 0.5247 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8772 - loss: 0.5146 - val_accuracy: 0.8750 - val_loss: 0.5129 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8936 - loss: 0.5027 - val_accuracy: 0.8972 - val_loss: 0.5010 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9000 - loss: 0.4918 - val_accuracy: 0.9056 - val_loss: 0.4893 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9054 - loss: 0.4832 - val_accuracy: 0.9111 - val_loss: 0.4776 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9169 - loss: 0.4706 - val_accuracy: 0.9417 - val_loss: 0.4659 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9186 - loss: 0.4609 - val_accuracy: 0.9528 - val_loss: 0.4539 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9310 - loss: 0.4495 - val_accuracy: 0.9639 - val_loss: 0.4418 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9373 - loss: 0.4346 - val_accuracy: 0.9639 - val_loss: 0.4296 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9493 - loss: 0.4250 - val_accuracy: 0.9667 - val_loss: 0.4174 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9523 - loss: 0.4120 - val_accuracy: 0.9667 - val_loss: 0.4054 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9489 - loss: 0.4048 - val_accuracy: 0.9722 - val_loss: 0.3933 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9587 - loss: 0.3897 - val_accuracy: 0.9722 - val_loss: 0.3815 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9604 - loss: 0.3770 - val_accuracy: 0.9722 - val_loss: 0.3697 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9622 - loss: 0.3666 - val_accuracy: 0.9722 - val_loss: 0.3581 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9605 - loss: 0.3590 - val_accuracy: 0.9722 - val_loss: 0.3465 - learning_rate: 0.0010\n",
            "Epoch 35/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9622 - loss: 0.3458 - val_accuracy: 0.9722 - val_loss: 0.3352 - learning_rate: 0.0010\n",
            "Epoch 36/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9685 - loss: 0.3357 - val_accuracy: 0.9778 - val_loss: 0.3238 - learning_rate: 0.0010\n",
            "Epoch 37/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9702 - loss: 0.3254 - val_accuracy: 0.9778 - val_loss: 0.3125 - learning_rate: 0.0010\n",
            "Epoch 38/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9700 - loss: 0.3156 - val_accuracy: 0.9778 - val_loss: 0.3013 - learning_rate: 0.0010\n",
            "Epoch 39/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9740 - loss: 0.3017 - val_accuracy: 0.9778 - val_loss: 0.2906 - learning_rate: 0.0010\n",
            "Epoch 40/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9726 - loss: 0.2942 - val_accuracy: 0.9778 - val_loss: 0.2798 - learning_rate: 0.0010\n",
            "Epoch 41/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9722 - loss: 0.2859 - val_accuracy: 0.9806 - val_loss: 0.2695 - learning_rate: 0.0010\n",
            "Epoch 42/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9722 - loss: 0.2765 - val_accuracy: 0.9806 - val_loss: 0.2596 - learning_rate: 0.0010\n",
            "Epoch 43/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9775 - loss: 0.2634 - val_accuracy: 0.9806 - val_loss: 0.2500 - learning_rate: 0.0010\n",
            "Epoch 44/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9761 - loss: 0.2557 - val_accuracy: 0.9806 - val_loss: 0.2406 - learning_rate: 0.0010\n",
            "Epoch 45/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9775 - loss: 0.2477 - val_accuracy: 0.9806 - val_loss: 0.2313 - learning_rate: 0.0010\n",
            "Epoch 46/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9796 - loss: 0.2374 - val_accuracy: 0.9833 - val_loss: 0.2223 - learning_rate: 0.0010\n",
            "Epoch 47/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9764 - loss: 0.2327 - val_accuracy: 0.9833 - val_loss: 0.2136 - learning_rate: 0.0010\n",
            "Epoch 48/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9812 - loss: 0.2185 - val_accuracy: 0.9833 - val_loss: 0.2054 - learning_rate: 0.0010\n",
            "Epoch 49/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9789 - loss: 0.2169 - val_accuracy: 0.9833 - val_loss: 0.1975 - learning_rate: 0.0010\n",
            "Epoch 50/50\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9811 - loss: 0.2051 - val_accuracy: 0.9833 - val_loss: 0.1900 - learning_rate: 0.0010\n"
          ]
        }
      ],
      "source": [
        "# 1. Data Loading and Preprocessing\n",
        "def load_and_preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    if 'Customer_ID' in data.columns:\n",
        "        data = data.drop('Customer_ID', axis=1)\n",
        "\n",
        "    # Handle NaN values in the target variable\n",
        "    data = data.dropna(subset=['churn'])\n",
        "\n",
        "    X = data.drop('churn', axis=1)\n",
        "    y = data['churn']\n",
        "\n",
        "    # Ensure y is integer type\n",
        "    y = y.astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "\n",
        "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "    if issparse(X_train_preprocessed):\n",
        "        X_train_preprocessed = X_train_preprocessed.toarray()\n",
        "    if issparse(X_test_preprocessed):\n",
        "        X_test_preprocessed = X_test_preprocessed.toarray()\n",
        "\n",
        "    return X_train_preprocessed, X_test_preprocessed, y_train, y_test, preprocessor\n",
        "\n",
        "# 2. Feature Engineering\n",
        "@tf.function\n",
        "def feature_engineering(X):\n",
        "    X_new = tf.concat([\n",
        "        X,\n",
        "        tf.expand_dims(X[:, 0] * X[:, 1], axis=1),\n",
        "        tf.expand_dims(tf.reduce_sum(X[:, :3], axis=1), axis=1),\n",
        "        tf.expand_dims(tf.reduce_mean(X[:, 3:], axis=1), axis=1)\n",
        "    ], axis=1)\n",
        "    return X_new\n",
        "\n",
        "# 3. LSTM with Attention\n",
        "def build_lstm_attention(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = LSTM(32, return_sequences=True)(inputs)  # Reduced from 64\n",
        "    x = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)  # Reduced from 8 heads, 64 key_dim\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dense(32, activation='relu')(x)  # Reduced from 64\n",
        "    x = Dropout(0.2)(x)  # Reduced from 0.3\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# 4. Base Models\n",
        "def train_base_models(X_train, y_train):\n",
        "    # XGBoost\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        max_depth=4,  # Reduced from 6\n",
        "        learning_rate=0.1,\n",
        "        n_estimators=100,  # Reduced from 1000\n",
        "        tree_method='hist',\n",
        "        device='cpu'  # Changed to CPU for compatibility\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # LightGBM\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        num_leaves=31,\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=100,  # Reduced from 1000\n",
        "        device='cpu'  # Changed to CPU for compatibility\n",
        "    )\n",
        "    lgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Neural Network\n",
        "    nn_model = tf.keras.Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Reduced from 256\n",
        "        Dropout(0.2),  # Reduced from 0.3\n",
        "        Dense(64, activation='relu'),  # Reduced from 128\n",
        "        Dropout(0.2),  # Reduced from 0.3\n",
        "        Dense(32, activation='relu'),  # Reduced from 64\n",
        "        Dropout(0.2),  # Reduced from 0.3\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    nn_model.fit(X_train, y_train, epochs=50, batch_size=512, validation_split=0.2,  # Reduced epochs from 100, increased batch size\n",
        "                 callbacks=[EarlyStopping(patience=5), ReduceLROnPlateau(patience=3)])\n",
        "\n",
        "    # SVM (with reduced sample size for faster computation)\n",
        "    sample_size = min(10000, X_train.shape[0])  # Use at most 10,000 samples\n",
        "    X_train_sample = X_train[:sample_size]\n",
        "    y_train_sample = y_train[:sample_size]\n",
        "    svm_model = SVC(kernel='rbf', C=1.0, probability=True)\n",
        "    svm_model.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "    return xgb_model, lgb_model, nn_model, svm_model, xgb_model.predict_proba(X_train)[:, 1], lgb_model.predict_proba(X_train)[:, 1], nn_model.predict(X_train).flatten(), svm_model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "# 5. Adaptive Ensemble\n",
        "def build_adaptive_ensemble(base_models, meta_features_shape):\n",
        "    base_inputs = [Input(shape=(1,)) for _ in range(len(base_models))]\n",
        "    meta_features_input = Input(shape=meta_features_shape)\n",
        "\n",
        "    concat = tf.keras.layers.concatenate(base_inputs + [meta_features_input])\n",
        "    x = Dense(32, activation='relu')(concat)  # Reduced from 64\n",
        "    x = Dense(16, activation='relu')(x)  # Reduced from 32\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_inputs + [meta_features_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 6. Online Learning Component\n",
        "class OnlineLearning(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, data_generator):\n",
        "        super(OnlineLearning, self).__init__()\n",
        "        self.data_generator = data_generator\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        X_online, y_online = next(self.data_generator)\n",
        "        self.model.train_on_batch(X_online, y_online)\n",
        "\n",
        "# 7. Transfer Learning (simplified)\n",
        "def apply_transfer_learning(model, new_data):\n",
        "    for layer in model.layers[:-2]:\n",
        "        layer.trainable = False\n",
        "    model.fit(new_data[0], new_data[1], epochs=5, batch_size=512)  # Reduced epochs from 10\n",
        "    return model\n",
        "\n",
        "# 8. Interpretability\n",
        "def interpret_model(model, X):\n",
        "    # Create a new model that outputs the last timestep\n",
        "    last_timestep_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\n",
        "    # Use the new model for SHAP explanation\n",
        "    explainer = shap.DeepExplainer(last_timestep_model, X[:100])  # Reduced sample size for SHAP\n",
        "    shap_values = explainer.shap_values(X[:100])\n",
        "    return shap_values\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Use GPU if available\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "\n",
        "    # Create a directory for checkpoints\n",
        "    os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    X_train, X_test, y_train, y_test, preprocessor = load_and_preprocess_data('telco_customer_churn.csv')\n",
        "\n",
        "    # Save preprocessor\n",
        "    joblib.dump(preprocessor, 'checkpoints/preprocessor.joblib')\n",
        "\n",
        "    # Feature engineering\n",
        "    X_train_eng = feature_engineering(tf.constant(X_train, dtype=tf.float32)).numpy()\n",
        "    X_test_eng = feature_engineering(tf.constant(X_test, dtype=tf.float32)).numpy()\n",
        "\n",
        "    # Reshape data for LSTM\n",
        "    X_train_lstm = X_train_eng.reshape((X_train_eng.shape[0], 1, X_train_eng.shape[1]))\n",
        "    X_test_lstm = X_test_eng.reshape((X_test_eng.shape[0], 1, X_test_eng.shape[1]))\n",
        "\n",
        "    # Build and train LSTM with Attention\n",
        "    lstm_model = build_lstm_attention((1, X_train_eng.shape[1]))\n",
        "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    lstm_checkpoint = ModelCheckpoint('checkpoints/lstm_model.keras', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "    lstm_model.fit(X_train_lstm, y_train,\n",
        "                   epochs=50, batch_size=512, validation_split=0.15,  # Reduced epochs from 100, increased batch size\n",
        "                   callbacks=[EarlyStopping(patience=5), lstm_checkpoint, ReduceLROnPlateau(patience=3)])\n",
        "\n",
        "    # Train base models\n",
        "    xgb_model, lgb_model, nn_model, svm_model, xgb_preds, lgb_preds, nn_preds, svm_preds = train_base_models(X_train_eng, y_train)\n",
        "\n",
        "    # Save base models\n",
        "    joblib.dump(xgb_model, 'checkpoints/xgb_model.joblib')\n",
        "    joblib.dump(lgb_model, 'checkpoints/lgb_model.joblib')\n",
        "    nn_model.save('checkpoints/nn_model.keras')\n",
        "\n",
        "    lstm_model.save('checkpoints/lstm_model_transfer.keras')\n",
        "    joblib.dump(svm_model, 'checkpoints/svm_model.joblib')\n",
        "\n",
        "    # Prepare inputs for adaptive ensemble\n",
        "    base_preds = [\n",
        "        xgb_model.predict_proba(X_train_eng)[:, 1],\n",
        "        lgb_model.predict_proba(X_train_eng)[:, 1],\n",
        "        nn_model.predict(X_train_eng).flatten(),\n",
        "        svm_model.predict_proba(X_train_eng)[:, 1]\n",
        "    ]\n",
        "    meta_features = np.column_stack(base_preds)\n",
        "\n",
        "    # Build and train adaptive ensemble\n",
        "    adaptive_ensemble = build_adaptive_ensemble([xgb_model, lgb_model, nn_model, svm_model], meta_features.shape[1:])\n",
        "\n",
        "    ensemble_checkpoint = ModelCheckpoint('checkpoints/adaptive_ensemble.keras', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "    adaptive_ensemble.fit([p.reshape(-1, 1) for p in base_preds] + [meta_features], y_train,\n",
        "                          epochs=50, batch_size=512, validation_split=0.2,  # Reduced epochs from 100, increased batch size\n",
        "                          callbacks=[ensemble_checkpoint, ReduceLROnPlateau(patience=3)])\n",
        "\n",
        "    # Online learning (simulated)\n",
        "def data_generator():\n",
        "    while True:\n",
        "        idx = np.random.randint(0, X_train_eng.shape[0], 32)\n",
        "        yield [p[idx].reshape(-1, 1) for p in base_preds] + [meta_features[idx]], y_train.iloc[idx]\n",
        "\n",
        "    online_learning_callback = OnlineLearning(data_generator())\n",
        "    adaptive_ensemble.fit([p.reshape(-1, 1) for p in base_preds] + [meta_features], y_train,\n",
        "                          epochs=5, callbacks=[online_learning_callback])  # Reduced epochs from 10\n",
        "\n",
        "    # Save updated adaptive ensemble\n",
        "    adaptive_ensemble.save('checkpoints/adaptive_ensemble_online.keras')\n",
        "\n",
        "    # Transfer learning (simulated with a subset of data)\n",
        "    new_data = (X_test_lstm[:100], y_test[:100])\n",
        "    lstm_model = apply_transfer_learning(lstm_model, new_data)\n",
        "\n",
        "    # Save transfer learned model\n",
        "    lstm_model.save('checkpoints/lstm_model_transfer.keras')\n",
        "\n",
        "    # Model interpretation (using a subset of data)\n",
        "    shap_values = interpret_model(lstm_model, X_test_lstm[:1000])\n",
        "\n",
        "    print(\"Model training and evaluation completed.\")\n",
        "\n",
        "    # Evaluate final model\n",
        "    ensemble_preds = adaptive_ensemble.predict([p.reshape(-1, 1) for p in [\n",
        "        xgb_model.predict_proba(X_test_eng)[:, 1],\n",
        "        lgb_model.predict_proba(X_test_eng)[:, 1],\n",
        "        nn_model.predict(X_test_eng).flatten(),\n",
        "        svm_model.predict_proba(X_test_eng)[:, 1]\n",
        "    ]] + [np.column_stack([\n",
        "        xgb_model.predict_proba(X_test_eng)[:, 1],\n",
        "        lgb_model.predict_proba(X_test_eng)[:, 1],\n",
        "        nn_model.predict(X_test_eng).flatten(),\n",
        "        svm_model.predict_proba(X_test_eng)[:, 1]\n",
        "    ])])\n",
        "\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "    y_pred = (ensemble_preds > 0.5).astype(int)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
